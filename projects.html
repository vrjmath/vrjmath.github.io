<!DOCTYPE html>

<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Benchmarking GNNs on Directed Graphs</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Viraj Shitole">

  <!-- LaTeX.css stylesheet -->

  <link rel="stylesheet" href="https://latex.vercel.app/style.css">
</head>

<body class="text-justify">

  <h1>Benchmarking GNNs on Directed Graphs</h1>
  <p class="author">Viraj Shitole</p>

  <div class="abstract">
    <h2>Abstract</h2>
    <p>
      Graph neural networks (GNNs) have achieved strong performance in learning biological structures, social networks, and citation graphs. However, these graphs are typically undirected, and the effectiveness of GNNs on directed graph datasets remains largely unexplored. In this project, we benchmark multiple GNN architectures on directed graph datasets, specifically source code graphs. We use the OpenCL DevMap dataset, which contains rich node features, directed edges, and graph-level labels capturing edge-device optimizations. By evaluating GNN performance, we quantify their ability to capture directed dependencies and structural asymmetries in real-world directed graphs.
    </p>
  </div>

  <h2>1) Dataset</h2>
  <p>
    We use the <strong>OpenCL DevMap dataset</strong>, a heterogeneous Compute Device Mapping dataset:
  </p>
  <ul>
    <li>Nodes represent lines of code or functions (Average: 845 nodes per graph)</li>
    <li>Edges represent function calls, data flows, or return calls (Average: 1573 edges per graph)</li>
    <li>Graph labels are binary: 0 for CPU, 1 for GPU</li>
    <li>Total of 680 labeled graphs</li>
    <li>Graphs are directed, reflecting true execution dependencies</li>
  </ul>

  <h2>2) Models</h2>
  <p>
    We evaluate three popular GNN architectures:
  </p>
  <ul>
    <li><strong>GraphSAGE</strong>: Aggregates information from neighbors using a learnable aggregation function.</li>
    <li><strong>Graph Isomorphism Network (GIN)</strong>: Uses a sum aggregator with MLP for expressive node updates.</li>
    <li><strong>Graph Convolutional Network (GCN)</strong>: Propagates normalized neighbor information.</li>
  </ul>
  <p>
    We also implement <strong>bidirectional variants</strong> for all three models, which process edges in both directions to capture reverse dependencies.
  </p>

  <h2>3) Experimental Setup</h2>
  <ul>
    <li>Cross-validation: 10-fold stratified</li>
    <li>Loss function: Binary cross-entropy with logits</li>
    <li>Optimizer: Adam</li>
    <li>Early stopping: Patience of 10 epochs</li>
    <li>Metrics: F1-score and ROC-AUC</li>
    <li>Graph handling: Option to treat graphs as undirected by adding reverse edges</li>
  </ul>
  <p>
    Evaluation metrics:
    <ul>
      <li>F1-score: harmonic mean of precision and recall</li>
      <li>ROC-AUC: measures the model’s ability to discriminate between CPU and GPU optimal graphs</li>
    </ul>
  </p>

  <h2>4) Results</h2>
  <p>
    Results are summarized in the table below (placeholders for values):
  </p>
  <table>
    <tr><th>Model</th><th>Direction</th><th>F1 (Val)</th><th>ROC-AUC (Val)</th><th>F1 (Test)</th><th>ROC-AUC (Test)</th></tr>
    <tr><td>GraphSAGE</td><td>Directed</td><td>X.XX</td><td>X.XX</td><td>X.XX</td><td>X.XX</td></tr>
    <tr><td>GraphSAGE</td><td>Bidirectional</td><td>X.XX</td><td>X.XX</td><td>X.XX</td><td>X.XX</td></tr>
    <tr><td>GIN</td><td>Directed</td><td>X.XX</td><td>X.XX</td><td>X.XX</td><td>X.XX</td></tr>
    <tr><td>GIN</td><td>Bidirectional</td><td>X.XX</td><td>X.XX</td><td>X.XX</td><td>X.XX</td></tr>
    <tr><td>GCN</td><td>Directed</td><td>X.XX</td><td>X.XX</td><td>X.XX</td><td>X.XX</td></tr>
    <tr><td>GCN</td><td>Bidirectional</td><td>X.XX</td><td>X.XX</td><td>X.XX</td><td>X.XX</td></tr>
  </table>

  <h2>5) Discussion</h2>
  <p>
    Directed edges contain valuable information for performance prediction in source code graphs. Bidirectional and undirected modeling are useful but may blur asymmetries in some datasets. GraphSAGE’s neighbor aggregation appears more stable than GIN’s MLP-based updates on this dataset.
  </p>
  <p>
    Future work includes exploring attention-based GNNs (e.g., GAT) for edge weighting, extending evaluation to larger heterogeneous code datasets, and investigating node and edge masking strategies to improve generalization.
  </p>

</body>
</html>
